{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_실습.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunghoByun/TensorflowDeveloperCertificate/blob/main/CNN_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9JDfOjgjXOi"
      },
      "source": [
        "# CNN 실습\n",
        "\n",
        "앞서 살펴본 예제를 바탕으로, 실제로 코드를 작성하는 실습을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KticUNEmQCir"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXe76aUDuGID"
      },
      "source": [
        "## Rock-Paper-Scissors 데이터셋 로드\n",
        "\n",
        "Rock-Paper-Scissors 데이터셋을 로드합니다.\n",
        "\n",
        "Rock-Paper-Scissors 데이터셋은 가위, 바위, 보 게임을 하는 손 이미지를 모은 데이터셋으로, 300x300x3 크기의 이미지들로 구성되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_I2pJLrqtaA"
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n",
        "urllib.request.urlretrieve(url, 'rps.zip')\n",
        "local_zip = 'rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mke4hFnhvu6S"
      },
      "source": [
        "## 로드한 데이터 확인\n",
        "\n",
        "예시로 몇 개의 데이터만 확인해보겠습니다.\n",
        "\n",
        "현재 경로의 tmp/rps 폴더 안에 paper, rock, scissors 폴더와 이미지가 생성된 것을 확인할 수 있습니다.\n",
        "\n",
        "Rock-Scissors-Paper Dataset의 세부적인 정보는 아래 링크에서 확인할 수 있습니다.\n",
        "\n",
        "https://www.tensorflow.org/datasets/catalog/rock_paper_scissors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEgYnX1Y_LT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc950a59-4d1e-48fc-d122-7d9780ceb892"
      },
      "source": [
        "!ls tmp/rps/* | head -20"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tmp/rps/paper:\n",
            "paper01-000.png\n",
            "paper01-001.png\n",
            "paper01-002.png\n",
            "paper01-003.png\n",
            "paper01-004.png\n",
            "paper01-005.png\n",
            "paper01-006.png\n",
            "paper01-007.png\n",
            "paper01-008.png\n",
            "paper01-009.png\n",
            "paper01-010.png\n",
            "paper01-011.png\n",
            "paper01-012.png\n",
            "paper01-013.png\n",
            "paper01-014.png\n",
            "paper01-015.png\n",
            "paper01-016.png\n",
            "paper01-017.png\n",
            "paper01-018.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_gxPrVawCRA"
      },
      "source": [
        "## **MISSION: 데이터 전처리**\n",
        "\n",
        "##**※ 실제 시험 문제가 이와 같은 형태로 출제됩니다.**\n",
        "\n",
        "이 데이터로 모델을 학습시키기 위해선 각종 전처리가 필요합니다.\n",
        "\n",
        "TFDS 패키지를 이용한 것이 아니므로 ImageDataGenerator를 이용하여 전처리를 진행해야 합니다.\n",
        "\n",
        "##**작성할 코드**\n",
        "\n",
        "1. ImageDataGenerator를 이용하여 픽셀값 normalize를 진행하며 학습데이터를 구성하십시오.\n",
        "\n",
        "2. 구성된 학습 데이터에 flow_from_directory를 이용하여 데이터 전처리 및 batch 사이즈를 지정하십시오.\n",
        "\n",
        "3. (선택) Train/Validation Split을 진행하십시오.\n",
        "\n",
        "4. (선택) ImageDataGenerator 과정에서 Augmentation 과정을 추가하십시오.\n",
        "\n",
        "3번과 4번의 경우에는 (선택)이므로 진행하지 않아도 문제 없습니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "ImageDataGenerator와 flow_from_directory에 관련한 상세한 자료는 아래 TensorFlow Document에서 확인하실 수 있습니다.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsAhCtsfM09r"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frQkvGl1iblF"
      },
      "source": [
        "TRAINING_DIR = \"tmp/rps/\"\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "    # TODO: 픽셀값 normalize와 학습데이터 구성\n",
        "    rescale = 1./255,\n",
        "    validation_split = 0.2\n",
        ")\n",
        "\n",
        "# TODO: 학습 데이터에 flow_from_directory를 이용하여 데이터 전처리 및 batch 사이즈 지정"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EECrqTW2THvg"
      },
      "source": [
        "#Augmentation 적용\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=5,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        " "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZJFUiuKMwOA",
        "outputId": "f2e9b272-ea5f-4c50-b98e-ef34ea6eb69a"
      },
      "source": [
        "training_data = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size = (256,256),\n",
        "    batch_size = 10,\n",
        "    class_mode = 'sparse',\n",
        "    subset = 'training'\n",
        ")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2016 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Y-PY7LNxXk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMb_1uVdNDab",
        "outputId": "069623bd-6028-4304-84d8-3c4935c962be"
      },
      "source": [
        "test_data = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size = (256,256),\n",
        "    batch_size = 10,\n",
        "    class_mode = 'sparse',\n",
        "    subset = 'validation'\n",
        ")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 504 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MdGXiyihcD"
      },
      "source": [
        "## **MISSION: 네트워크 정의**\n",
        "\n",
        "##**※ 실제 시험 문제가 이와 같은 형태로 출제됩니다.**\n",
        "\n",
        "위에서 변환한 데이터로 학습할 네트워크를 정의합니다.\n",
        "\n",
        "##**작성할 코드**\n",
        "\n",
        "Image Classification에 잘 동작하는 네트워크를 설계해보세요. 마지막 레이어는 그대로 둔 채, 앞 부분의 레이어들을 추가하시면 됩니다.\n",
        "\n",
        "Validation Dataset이 있다면, Validation Accuracy가 80% 이상이 되도록 설계하시면 됩니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "Conv1D, MaxPooling2D, Dense 레이어를 중심으로 구현하면 좋습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rySUAYQPihMP"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # TODO: Add Layers\n",
        "    tf.keras.layers.Conv2D(16,(2,2),activation='relu', input_shape = (256,256,3)),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(32,(2,2),activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Conv2D(64,(2,2),activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(rate = 0.3),\n",
        "    tf.keras.layers.Dense(3, activation= 'softmax')\n",
        "])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXTDGvdeOiXi",
        "outputId": "ee87e290-5ddd-441a-c0e9-3ebeeb66a60d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 255, 255, 16)      208       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 127, 127, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 126, 126, 32)      2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 62, 62, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 61504)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               7872640   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 7,900,083\n",
            "Trainable params: 7,900,083\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuVSUwxKnNDq"
      },
      "source": [
        "## **MISSION: 네트워크 학습**\n",
        "\n",
        "##**※ 실제 시험 문제가 이와 같은 형태로 출제됩니다.**\n",
        "\n",
        "위에서 설계한 네트워크 구조를 바탕으로 compile 및 학습을 진행합니다.\n",
        "\n",
        "##**작성할 코드**\n",
        "\n",
        "설계한 네트워크에 적절한 Loss, Optimizer, Metrics을 지정하여 모델을 Compile 및 학습을 실시합니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "Validation Dataset이 있다면, Validation accuracy가 80% 이상 나오도록 위의 네트워크 구조 및 학습 epoch 수를 변경하시면 좋습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXqXUViDtO0x"
      },
      "source": [
        "# TODO: Compile and Training\n",
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
        "    metrics = ['sparse_categorical_accuracy','accuracy']\n",
        ")\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wrbALYrWijT",
        "outputId": "eeff86ce-99d5-4f41-8e75-7c5f02b16e4c"
      },
      "source": [
        "model.fit(training_data, epochs= 20, validation_data= test_data)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "202/202 [==============================] - 39s 190ms/step - loss: 1.0546 - sparse_categorical_accuracy: 0.4097 - accuracy: 0.4097 - val_loss: 0.8142 - val_sparse_categorical_accuracy: 0.5556 - val_accuracy: 0.5556\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.4330 - sparse_categorical_accuracy: 0.8104 - accuracy: 0.8104 - val_loss: 0.7000 - val_sparse_categorical_accuracy: 0.6806 - val_accuracy: 0.6806\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.2553 - sparse_categorical_accuracy: 0.9070 - accuracy: 0.9070 - val_loss: 1.2217 - val_sparse_categorical_accuracy: 0.6250 - val_accuracy: 0.6250\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 38s 188ms/step - loss: 0.1832 - sparse_categorical_accuracy: 0.9334 - accuracy: 0.9334 - val_loss: 0.5276 - val_sparse_categorical_accuracy: 0.8254 - val_accuracy: 0.8254\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 38s 190ms/step - loss: 0.1573 - sparse_categorical_accuracy: 0.9494 - accuracy: 0.9494 - val_loss: 0.5863 - val_sparse_categorical_accuracy: 0.8095 - val_accuracy: 0.8095\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9709 - accuracy: 0.9709 - val_loss: 0.7678 - val_sparse_categorical_accuracy: 0.7956 - val_accuracy: 0.7956\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9738 - accuracy: 0.9738 - val_loss: 0.8368 - val_sparse_categorical_accuracy: 0.7698 - val_accuracy: 0.7698\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0884 - sparse_categorical_accuracy: 0.9693 - accuracy: 0.9693 - val_loss: 0.5675 - val_sparse_categorical_accuracy: 0.8175 - val_accuracy: 0.8175\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 38s 190ms/step - loss: 0.0722 - sparse_categorical_accuracy: 0.9731 - accuracy: 0.9731 - val_loss: 0.8680 - val_sparse_categorical_accuracy: 0.6925 - val_accuracy: 0.6925\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0747 - sparse_categorical_accuracy: 0.9718 - accuracy: 0.9718 - val_loss: 0.7463 - val_sparse_categorical_accuracy: 0.8214 - val_accuracy: 0.8214\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0541 - sparse_categorical_accuracy: 0.9817 - accuracy: 0.9817 - val_loss: 0.5670 - val_sparse_categorical_accuracy: 0.8591 - val_accuracy: 0.8591\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0450 - sparse_categorical_accuracy: 0.9815 - accuracy: 0.9815 - val_loss: 0.5910 - val_sparse_categorical_accuracy: 0.8512 - val_accuracy: 0.8512\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 38s 190ms/step - loss: 0.0345 - sparse_categorical_accuracy: 0.9882 - accuracy: 0.9882 - val_loss: 0.4248 - val_sparse_categorical_accuracy: 0.8690 - val_accuracy: 0.8690\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0720 - sparse_categorical_accuracy: 0.9734 - accuracy: 0.9734 - val_loss: 0.7676 - val_sparse_categorical_accuracy: 0.7520 - val_accuracy: 0.7520\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0813 - sparse_categorical_accuracy: 0.9709 - accuracy: 0.9709 - val_loss: 0.6200 - val_sparse_categorical_accuracy: 0.8234 - val_accuracy: 0.8234\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 38s 190ms/step - loss: 0.0583 - sparse_categorical_accuracy: 0.9837 - accuracy: 0.9837 - val_loss: 0.5136 - val_sparse_categorical_accuracy: 0.8651 - val_accuracy: 0.8651\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0306 - sparse_categorical_accuracy: 0.9883 - accuracy: 0.9883 - val_loss: 0.5193 - val_sparse_categorical_accuracy: 0.8532 - val_accuracy: 0.8532\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 38s 190ms/step - loss: 0.0584 - sparse_categorical_accuracy: 0.9824 - accuracy: 0.9824 - val_loss: 0.7246 - val_sparse_categorical_accuracy: 0.8075 - val_accuracy: 0.8075\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 38s 189ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9852 - accuracy: 0.9852 - val_loss: 0.6507 - val_sparse_categorical_accuracy: 0.8135 - val_accuracy: 0.8135\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 38s 190ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9893 - accuracy: 0.9893 - val_loss: 0.7342 - val_sparse_categorical_accuracy: 0.8155 - val_accuracy: 0.8155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa4700fa890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_NzO65mZkaT",
        "outputId": "c580ac47-19e8-4516-ba62-d7011f821651"
      },
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51/51 [==============================] - 7s 145ms/step - loss: 0.6770 - sparse_categorical_accuracy: 0.8115 - accuracy: 0.8115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6769545674324036, 0.8115079402923584, 0.8115079402923584]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}